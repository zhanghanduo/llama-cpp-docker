version: "3.9"
services:
  llama:
    image: llama-cpp-docker
    environment:
      - GGML_CUDA_NO_PINNED=1
      - LLAMA_CTX_SIZE=2048
      - LLAMA_MODEL=/models/bun_mistral_7b_v2.Q4_K_M.gguf
      - LLAMA_N_GPU_LAYERS=99
    volumes:
      - ./models:/models
    ports:
      - target: 18080
        published: 18080
        mode: host
